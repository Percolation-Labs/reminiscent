# REM Stack - Kustomize Deployment

This directory contains the complete REM stack organized using Kustomize for multi-environment deployment with semantic versioning.

## Deployment Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      REM Application Stack                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Namespace: rem-app
â”œâ”€â”€ rem-api (Deployment)
â”‚   â”œâ”€â”€ FastAPI server (REST + streaming)
â”‚   â”œâ”€â”€ MCP server mounted at /api/v1/mcp (NOT separate!)
â”‚   â”œâ”€â”€ HPA: 2-10 replicas (CPU-based)
â”‚   â”œâ”€â”€ PDB: min 1 available
â”‚   â””â”€â”€ External Secrets: LLM API keys
â”‚
â””â”€â”€ file-processor (Deployment)
    â”œâ”€â”€ KEDA ScaledObject: 0-20 replicas (SQS-based)
    â”œâ”€â”€ Spot instances preferred
    â””â”€â”€ Pod anti-affinity

Namespace: postgres (deployed separately)
â””â”€â”€ CloudNativePG Cluster
    â”œâ”€â”€ PostgreSQL 18 + pgvector
    â”œâ”€â”€ Init SQL via ConfigMap (generated)
    â””â”€â”€ Backup to S3

ConfigMaps (rem-app):
â”œâ”€â”€ rem-api-config (static)
â””â”€â”€ rem-version-info (generated by Kustomize)
```

## Kustomize Structure

```
rem-stack/
â”œâ”€â”€ components/                   # Individual components
â”‚   â”œâ”€â”€ api/                     # rem-api (FastAPI + MCP)
â”‚   â”œâ”€â”€ worker/                  # file-processor (KEDA)
â”‚   â””â”€â”€ postgres/                # PostgreSQL (separate namespace)
â”œâ”€â”€ base/                        # Base configuration (namespace: rem-app)
â”‚   â””â”€â”€ kustomization.yaml      # References api + worker components
â”œâ”€â”€ overlays/
â”‚   â”œâ”€â”€ staging/                 # Staging environment (v1.2.3-rc.1)
â”‚   â”‚   â””â”€â”€ kustomization.yaml
â”‚   â””â”€â”€ prod/                    # Production environment (v1.2.3)
â”‚       â””â”€â”€ kustomization.yaml
â”œâ”€â”€ argocd-staging.yaml         # ArgoCD Application for staging
â”œâ”€â”€ argocd-prod.yaml            # ArgoCD Application for production
â””â”€â”€ README.md                    # This file
```

## Architecture

The REM stack consists of **2 deployments** in the `rem-app` namespace:

1. **rem-api**: FastAPI server with **MCP server mounted** (not separate)
2. **file-processor**: KEDA-scaled worker for file processing

**Important**: The MCP (Model Context Protocol) server is **not a separate deployment**. It is mounted as part of the rem-api FastAPI application at `/api/v1/mcp`.

## Components Included

### rem-app Namespace (2 Deployments)

- **rem-api** (Deployment)
  - FastAPI REST and streaming endpoints
  - **MCP server mounted** at `/api/v1/mcp`
  - OpenTelemetry instrumentation (conditional)
  - HPA for CPU-based scaling (2-10 replicas)
  - PDB for high availability
  - External Secrets for LLM API keys
  - Ingress for external access

- **file-processor** (Deployment)
  - **KEDA-scaled** worker (0-20 replicas based on SQS)
  - Background file processing (S3 â†’ process â†’ S3)
  - Pod Identity/IRSA for AWS access
  - Spot instances preferred (cost optimization)
  - Pod anti-affinity for distribution

### postgres Namespace (Deployed Separately)

- **PostgreSQL** (CloudNativePG)
  - Database cluster with pgvector extension
  - SQL init scripts via ConfigMap (generated)
  - Scheduled backups to S3

## ConfigMaps

The stack uses **2 ConfigMaps** in the `rem-app` namespace:

1. **rem-api-config**
   - Static configuration (AWS region, log level, OTEL endpoint)
   - Defined in `components/api/configmap.yaml`

2. **rem-version-info**
   - Generated by Kustomize with version metadata
   - Contains: `VERSION`, `GIT_COMMIT`, `BUILD_DATE`, `ENVIRONMENT`
   - Defined in `base/kustomization.yaml` as `configMapGenerator`

The PostgreSQL init ConfigMap (`rem-postgres-init-sql`) is in the `postgres` namespace and deployed separately.

## KEDA Autoscaling

### How It Works

The `file-processor` worker automatically scales based on SQS queue depth:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         KEDA Scaling Flow                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Files uploaded to S3
   â””â”€> S3 Event Notification â†’ SQS Queue (rem-file-processing)

2. KEDA Operator (every 10 seconds)
   â””â”€> Polls SQS via IRSA role
   â””â”€> Gets ApproximateNumberOfMessages
   â””â”€> Calculates: desired_replicas = ceil(messages / queueLength)
   â””â”€> Updates HPA target

3. HorizontalPodAutoscaler
   â””â”€> Scales Deployment based on KEDA's calculation
   â””â”€> Respects min (0) and max (20) replica limits

4. Kubernetes Deployment
   â””â”€> Creates/destroys pods as needed
   â””â”€> Pods use file-processor ServiceAccount (Pod Identity/IRSA)

5. File Processor Pods
   â””â”€> Pull messages from SQS (via IRSA role)
   â””â”€> Download files from S3 (via IRSA role)
   â””â”€> Process files
   â””â”€> Delete messages from SQS
   â””â”€> Upload results to S3

6. Karpenter (optional)
   â””â”€> Provisions nodes if pods are pending
   â””â”€> Deprovisions nodes when pods terminate
```

**Scaling Formula:**
```
Desired Replicas = ceil(Queue Messages / queueLength)

Examples:
- 0 messages   â†’ 0 pods (scale to zero!)
- 15 messages  â†’ 2 pods (15/10 = 1.5, ceil = 2)
- 50 messages  â†’ 5 pods
- 200+ messages â†’ 20 pods (max)
```

### Configuration

```yaml
# file-processor/keda-scaledobject.yaml
spec:
  scaleTargetRef:
    name: file-processor

  minReplicaCount: 0        # Scale to ZERO when idle
  maxReplicaCount: 20       # Max 20 pods
  pollingInterval: 10       # Check SQS every 10 seconds
  cooldownPeriod: 60        # Wait 60s before scaling down

  triggers:
    - type: aws-sqs-queue
      metadata:
        queueURL: https://sqs.us-east-1.amazonaws.com/ACCOUNT_ID/rem-file-processing
        queueLength: "10"   # 1 pod per 10 messages
        identityOwner: operator  # KEDA operator polls SQS
```

### Scaling Behavior

**Scale Up (Aggressive):**
- Immediate (no stabilization window)
- Doubles pods every 15s OR adds 4 pods (whichever is more)
- Example: 1 â†’ 2 â†’ 4 â†’ 8 â†’ 16 â†’ 20 pods in ~90s

**Scale Down (Gradual):**
- 5-minute stabilization window
- Max 50% reduction per minute
- Example: 20 â†’ 10 â†’ 5 â†’ 3 â†’ 2 â†’ 1 â†’ 0 over ~6 minutes

### IAM Permissions

**Two separate roles are required:**

**1. KEDA Operator Role** (reads queue metrics)
```json
{
  "Statement": [{
    "Effect": "Allow",
    "Action": [
      "sqs:GetQueueAttributes",
      "sqs:GetQueueUrl"
    ],
    "Resource": "arn:aws:sqs:*:*:rem-*"
  }]
}
```

**2. File Processor Pod Role** (via Pod Identity/IRSA)
```json
{
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "sqs:ReceiveMessage",
        "sqs:DeleteMessage",
        "sqs:GetQueueAttributes"
      ],
      "Resource": "arn:aws:sqs:*:*:rem-file-processing"
    },
    {
      "Effect": "Allow",
      "Action": ["s3:GetObject", "s3:PutObject"],
      "Resource": "arn:aws:s3:::rem-files/*"
    }
  ]
}
```

The pod role is attached via ServiceAccount annotation:
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: file-processor
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/rem-file-processor-role
```

### ArgoCD Integration

**Critical:** ArgoCD must ignore KEDA-managed fields to prevent sync conflicts.

```yaml
# argocd-staging.yaml / argocd-prod.yaml
spec:
  ignoreDifferences:
    # KEDA manages replica count - don't sync back to manifest value
    - group: apps
      kind: Deployment
      jsonPointers:
        - /spec/replicas

    # KEDA manages ScaledObject triggers dynamically
    - group: keda.sh
      kind: ScaledObject
      jsonPointers:
        - /spec/triggers
```

**Why this is needed:**
- Manifest specifies `replicas: 0`
- KEDA scales to (e.g.) 10 replicas based on queue
- Without `ignoreDifferences`, ArgoCD sees drift and syncs back to 0
- With `ignoreDifferences`, ArgoCD ignores replica count changes

### Monitoring KEDA

```bash
# Check ScaledObject status
kubectl get scaledobject file-processor-scaler -n rem-app

# Detailed metrics
kubectl describe scaledobject file-processor-scaler -n rem-app

# View HPA created by KEDA
kubectl get hpa -n rem-app

# Watch scaling in real-time
watch kubectl get pods -n rem-app -l app=file-processor

# KEDA operator logs
kubectl logs -f -n keda deployment/keda-operator

# Check SQS queue depth
aws sqs get-queue-attributes \
  --queue-url https://sqs.us-east-1.amazonaws.com/ACCOUNT_ID/rem-file-processing \
  --attribute-names ApproximateNumberOfMessages
```

### Testing KEDA Scaling

```bash
# Send test messages to SQS
for i in {1..50}; do
  aws sqs send-message \
    --queue-url https://sqs.us-east-1.amazonaws.com/ACCOUNT_ID/rem-file-processing \
    --message-body "{\"file\":\"test-$i.pdf\"}"
done

# Watch KEDA scale up
watch kubectl get pods -n default -l app=file-processor

# Expected: 5 pods (50 messages / 10 per pod)
```

### Environment-Specific Scaling

Customize scaling per environment using overlays:

```yaml
# overlays/prod/keda-patch.yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: file-processor-scaler
spec:
  maxReplicaCount: 50  # Higher limit for production
  triggers:
    - type: aws-sqs-queue
      metadata:
        queueLength: "5"  # More aggressive: 1 pod per 5 messages
```

Add to `overlays/prod/kustomization.yaml`:
```yaml
patchesStrategicMerge:
  - keda-patch.yaml
```

### Cost Optimization

- **Scale to zero**: No cost when queue is empty
- **Fast scale-up**: Handle bursts without over-provisioning
- **Gradual scale-down**: Avoid thrashing and instability
- **Karpenter integration**: Nodes scale down when pods terminate
- **Spot instances**: Use Karpenter spot node pools for workers

## Semantic Versioning Strategy

### Staging

Release candidates and pre-releases:
- `v1.2.3-rc.1`, `v1.2.3-rc.2` (release candidates)
- `v1.2.3-beta.1` (beta)
- `v1.2.3-alpha.1` (alpha)

### Production

Stable releases only:
- `v1.2.3` (stable)
- `v1.2.4` (patch)
- `v1.3.0` (minor)
- `v2.0.0` (major)

## Deployment Options

### Option 1: Direct kubectl + Kustomize

Apply manifests directly for testing or manual control.

#### Deploy to Staging

```bash
# Set environment variables
export ECR_REGISTRY="123456789012.dkr.ecr.us-east-1.amazonaws.com"
export IMAGE_TAG="v1.2.3-rc.1"
export GIT_COMMIT=$(git rev-parse --short HEAD)
export BUILD_DATE=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

# Preview what will be deployed
kubectl kustomize manifests/application/rem-stack/overlays/staging

# Apply to cluster
kubectl apply -k manifests/application/rem-stack/overlays/staging

# Verify deployment
kubectl get pods -n rem-staging
kubectl get applications -n argocd  # If using ArgoCD
```

#### Deploy to Production

```bash
# Set environment variables
export ECR_REGISTRY="123456789012.dkr.ecr.us-east-1.amazonaws.com"
export IMAGE_TAG="v1.2.3"  # Stable version only
export GIT_COMMIT=$(git rev-parse --short HEAD)
export BUILD_DATE=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

# Preview what will be deployed
kubectl kustomize manifests/application/rem-stack/overlays/prod

# Apply to cluster
kubectl apply -k manifests/application/rem-stack/overlays/prod

# Verify deployment
kubectl get pods -n rem-prod
```

#### Update Image Version

```bash
# Navigate to overlay
cd manifests/application/rem-stack/overlays/staging

# Update image tag
kustomize edit set image \
  rem-api=$ECR_REGISTRY/rem-api:v1.2.4-rc.1 \
  rem-mcp=$ECR_REGISTRY/rem-mcp:v1.2.4-rc.1 \
  rem-worker=$ECR_REGISTRY/rem-worker:v1.2.4-rc.1

# Apply
kubectl apply -k .
```

### Option 2: ArgoCD GitOps (Recommended)

Use ArgoCD for automated continuous deployment from Git.

#### Setup

1. **Fork this repository**
   ```bash
   # Fork github.com/YOUR-ORG/remstack to your organization
   ```

2. **Update ArgoCD Application manifests**
   ```bash
   # Edit argocd-staging.yaml and argocd-prod.yaml
   vim manifests/application/rem-stack/argocd-staging.yaml

   # Update:
   spec.source.repoURL: https://github.com/YOUR-ORG/remstack.git
   spec.source.targetRevision: staging  # or main for prod
   ```

3. **Deploy ArgoCD Applications**
   ```bash
   # Deploy staging application
   kubectl apply -f manifests/application/rem-stack/argocd-staging.yaml

   # Deploy production application
   kubectl apply -f manifests/application/rem-stack/argocd-prod.yaml
   ```

#### Monitor Deployments

```bash
# Check application status
kubectl get applications -n argocd

# Get detailed status
argocd app get rem-stack-staging
argocd app get rem-stack-prod

# Watch sync progress
argocd app sync rem-stack-staging --watch

# View application in UI
kubectl port-forward svc/argocd-server -n argocd 8080:443
# Open https://localhost:8080
```

#### Promotion Workflow

1. **Build and tag images**
   ```bash
   # Build with semantic version
   docker build -t $ECR_REGISTRY/rem-api:v1.2.3-rc.1 .
   docker push $ECR_REGISTRY/rem-api:v1.2.3-rc.1
   ```

2. **Update staging overlay**
   ```bash
   cd manifests/application/rem-stack/overlays/staging

   # Update kustomization.yaml images
   kustomize edit set image \
     rem-api=$ECR_REGISTRY/rem-api:v1.2.3-rc.1

   # Commit and push
   git add kustomization.yaml
   git commit -m "chore(staging): bump version to v1.2.3-rc.1"
   git push origin staging
   ```

3. **ArgoCD auto-syncs staging** (if automated sync enabled)

4. **Test in staging**
   ```bash
   # Verify deployment
   kubectl get pods -n rem-staging

   # Check application health
   kubectl get applications rem-stack-staging -n argocd

   # Run tests
   kubectl exec -it deployment/rem-api -n rem-staging -- curl http://localhost:8000/health
   ```

5. **Promote to production**
   ```bash
   # Tag stable release
   docker tag $ECR_REGISTRY/rem-api:v1.2.3-rc.1 $ECR_REGISTRY/rem-api:v1.2.3
   docker push $ECR_REGISTRY/rem-api:v1.2.3

   # Update production overlay
   cd manifests/application/rem-stack/overlays/prod
   kustomize edit set image rem-api=$ECR_REGISTRY/rem-api:v1.2.3

   # Commit and push
   git add kustomization.yaml
   git commit -m "chore(prod): promote v1.2.3 to production"
   git push origin main
   ```

6. **Sync production** (manual or automated)
   ```bash
   # If selfHeal: false (manual sync required)
   argocd app sync rem-stack-prod

   # If selfHeal: true, ArgoCD auto-syncs
   ```

## Environment Comparison

| Feature | Staging | Production |
|---------|---------|------------|
| **Replicas** | 2 (base) | 3 |
| **PostgreSQL instances** | 3 (base) | 3 |
| **Storage size** | 50Gi (base) | 100Gi |
| **Memory (API)** | 4Gi (base) | 4Gi |
| **CPU (API)** | 2 cores (base) | 2 cores |
| **Log level** | Base config | INFO |
| **Image tags** | RC/beta/alpha | Stable only |
| **Auto-sync** | Yes | No (manual) |
| **Self-heal** | Yes | No |

## Configuration Management

### Environment Variables

Set in overlay `kustomization.yaml`:

```yaml
configMapGenerator:
  - name: rem-version-info
    behavior: merge
    literals:
      - VERSION=${IMAGE_TAG}
      - ENVIRONMENT=staging
      - GIT_COMMIT=${GIT_COMMIT}
      - BUILD_DATE=${BUILD_DATE}
```

### Secrets

Managed via External Secrets Operator:
- **AWS Parameter Store** for LLM API keys
- **AWS Secrets Manager** for database credentials
- **IRSA/Pod Identity** for AWS service access

## Verification

### Check All Components

```bash
# Staging
kubectl get all -n rem-staging
kubectl get cluster -n rem-staging  # PostgreSQL
kubectl get scaledobject -n rem-staging  # KEDA

# Production
kubectl get all -n rem-prod
kubectl get cluster -n rem-prod
kubectl get scaledobject -n rem-prod
```

### Check Version Info

```bash
# View version ConfigMap
kubectl get configmap rem-version-info -n rem-staging -o yaml

# Check deployment image tags
kubectl get deployment rem-api -n rem-staging -o jsonpath='{.spec.template.spec.containers[0].image}'
```

### Check Database

```bash
# Verify SQL migrations applied
kubectl exec -it rem-postgres-1 -n rem-staging -- psql -d remdb -c "SELECT * FROM migration_status();"

# Check PostgreSQL cluster status
kubectl get cluster rem-postgres -n rem-staging
```

## Rollback

### Via ArgoCD

```bash
# View history
argocd app history rem-stack-staging

# Rollback to previous revision
argocd app rollback rem-stack-staging <REVISION>
```

### Via kubectl

```bash
# Rollback deployment
kubectl rollout undo deployment/rem-api -n rem-staging

# View rollout status
kubectl rollout status deployment/rem-api -n rem-staging
```

### Via Git

```bash
# Revert commit
git revert <commit-hash>
git push

# ArgoCD will auto-sync the revert
```

## Troubleshooting

### KEDA Not Scaling

**Symptom:** Pods stay at 0 even with messages in queue

```bash
# 1. Check ScaledObject status
kubectl describe scaledobject file-processor-scaler -n default

# Look for:
# - Active: True
# - ScalerActive: True
# - Conditions

# 2. Check KEDA operator logs
kubectl logs -f -n keda deployment/keda-operator

# 3. Verify KEDA can access SQS
kubectl get scaledobject file-processor-scaler -n default -o yaml | grep -A 5 conditions

# 4. Check IAM permissions
kubectl describe sa keda-operator -n keda
# Should have IRSA annotation with proper role
```

**Common issues:**
- KEDA operator missing SQS permissions
- Wrong queue URL in ScaledObject
- AWS region mismatch
- ScaledObject in wrong namespace

### ArgoCD Constantly Out of Sync

**Symptom:** ArgoCD shows "OutOfSync" for file-processor deployment

```bash
# Check if ArgoCD is fighting with KEDA
kubectl get deployment file-processor -n default -o jsonpath='{.spec.replicas}'
# If this differs from manifest (0), ArgoCD needs ignoreDifferences

# Verify ignoreDifferences is set
kubectl get application rem-stack-staging -n argocd -o yaml | grep -A 10 ignoreDifferences
```

**Fix:** Ensure ArgoCD Application has:
```yaml
ignoreDifferences:
  - group: apps
    kind: Deployment
    jsonPointers:
      - /spec/replicas
```

**Why this happens:**
- Manifest: `replicas: 0`
- KEDA scales to: `replicas: 5`
- ArgoCD sees drift: "Should be 0, but it's 5!"
- ArgoCD syncs back to 0
- KEDA scales back to 5
- Infinite loop ðŸ”„

### Pods Not Processing Messages

**Symptom:** KEDA scales up but pods don't process SQS messages

```bash
# 1. Check pod logs
kubectl logs -f -n default -l app=file-processor

# 2. Verify Pod Identity/IRSA
kubectl describe sa file-processor -n default
# Should have: eks.amazonaws.com/role-arn

# 3. Check pod environment variables
kubectl exec -it deployment/file-processor -n default -- env | grep AWS

# 4. Test SQS access from pod
kubectl exec -it deployment/file-processor -n default -- \
  aws sqs receive-message --queue-url https://sqs.us-east-1.amazonaws.com/ACCOUNT_ID/rem-file-processing
```

**Common issues:**
- Missing ServiceAccount annotation
- Wrong IAM role ARN
- IAM trust policy doesn't allow Pod Identity
- Missing SQS permissions in pod's IAM role

### Kustomize Build Failed

```bash
# Validate kustomization.yaml
kubectl kustomize manifests/application/rem-stack/overlays/staging

# Check for syntax errors
kustomize build manifests/application/rem-stack/overlays/staging
```

### ArgoCD Out of Sync

```bash
# Force sync
argocd app sync rem-stack-staging --force

# Refresh app
argocd app get rem-stack-staging --refresh
```

### Image Pull Errors

```bash
# Check ECR credentials
kubectl get secret -n rem-staging

# Verify IRSA role
kubectl describe sa rem-api -n rem-staging

# Check image exists
aws ecr describe-images --repository-name rem-api --image-ids imageTag=v1.2.3
```

## Best Practices

1. **Semantic Versioning**
   - Use semver for all releases
   - Test RCs in staging before promoting to prod
   - Never use `latest` tag in production

2. **GitOps Workflow**
   - All changes via Git commits
   - Use branch protection for main/production
   - Require PR reviews for production changes

3. **Testing Strategy**
   - Deploy to staging first
   - Run automated tests
   - Manual verification before prod promotion

4. **Monitoring**
   - Check ArgoCD sync status regularly
   - Monitor application health in ArgoCD UI
   - Set up alerts for sync failures

5. **Documentation**
   - Document all version changes in Git commits
   - Use conventional commits (feat, fix, chore)
   - Tag releases in Git

## References

- [Kustomize Documentation](https://kubectl.docs.kubernetes.io/references/kustomize/)
- [ArgoCD Documentation](https://argo-cd.readthedocs.io/)
- [Semantic Versioning](https://semver.org/)
- [CloudNativePG](https://cloudnative-pg.io/)
- [KEDA](https://keda.sh/)
